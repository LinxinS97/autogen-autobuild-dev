{"id": "7", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Use one-hot encoding for the 'Sex' and 'Embarked' features. Use the \"linear regression\" model provided by the sklearn library in Python."}, "file.txt": {"__FILE__": "../files/test_ave.csv"}, "expected_answer.txt": {"__ANSWER__": "@prediction_accuracy[0.78]"}, "format.txt": {"__FORMAT__": "@prediction_accuracy[accuracy], where \"accuracy\" is a float number rounded to 2 decimal places and has a range of 0.0 to 1.0."}, "question.txt": {"__QUESTION__": "Apply the linear regression algorithm from the sklearn library to predict whether a passenger survived or not based on the features 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', and 'Embarked'. Encode 'Sex' and 'Embarked' to numerical values before applying the model. Split the dataset into a training set (80%) and a testing set (20%), train the model on the training set, and evaluate its performance on the testing set using the accuracy score. Ensure that the train_test_split function's random_state parameter is set to 42 for consistency."}}}
{"id": "66", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between wage and the exper. Assess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.05. Report the p-value associated with the correlation test. Consider the relationship to be linear if the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5. Consider the relationship to be nonlinear if the p-value is less than 0.05 and the absolute value of r is less than 0.5. If the p-value is greater than or equal to 0.05, report that there is no significant correlation."}, "file.txt": {"__FILE__": "../files/beauty and the labor market.csv"}, "expected_answer.txt": {"__ANSWER__": "@correlation_coefficient[0.23],@relationship_type[nonlinear]"}, "format.txt": {"__FORMAT__": "@correlation_coefficient[r_value] @p_value[p_value] @relationship_type[relationship_type] where \"r_value\" is a number between -1 and 1, rounded to two decimal places. where \"p_value\" is a number between 0 and 1, rounded to four decimal places. where \"relationship_type\" is a string that can either be \"linear\", \"nonlinear\", or \"none\" based on the conditions specified in the constraints."}, "question.txt": {"__QUESTION__": "Calculate the correlation between the wage column and the exper column."}}}
{"id": "73", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Use the Pearson correlation coefficient for computation. Round the result to 2 decimal places."}, "file.txt": {"__FILE__": "../files/microsoft.csv"}, "expected_answer.txt": {"__ANSWER__": "@correlation_coefficient[1.0]"}, "format.txt": {"__FORMAT__": "@correlation_coefficient[correlation_coefficient]\\nwhere \"correlation_coefficient\" is a floating-point number rounded to 2 decimal places."}, "question.txt": {"__QUESTION__": "Calculate the correlation coefficient between the \"High\" and \"Low\" columns."}}}
{"id": "137", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "The logistic regression model should be implemented with scikit-learn\u2019s LogisticRegression with default parameters. Use the 'IsAlone' feature and 'Survived' as the output variable. The model should be trained using a 70:30 train-test split, balancing the class weights. Use a random seed of 42 for reproducibility."}, "file.txt": {"__FILE__": "../files/titanic.csv"}, "expected_answer.txt": {"__ANSWER__": "@model_score[0.61]"}, "format.txt": {"__FORMAT__": "@model_score[model_accuracy] where 'model_accuracy' is a number between 0 and 1, rounded to 2 decimal places, representing the accuracy of the model on the test set."}, "question.txt": {"__QUESTION__": "Perform feature engineering by creating a new binary feature called \"IsAlone\" that indicates whether a passenger is traveling alone or with family. Use the \"SibSp\" and \"Parch\" columns to determine if a passenger has any accompanying family members. Then, train a logistic regression machine learning model using the new feature and the Survival rate as the output variable."}}}
{"id": "179", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Use only passengers that survived and were in the first class. Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between age and fare."}, "file.txt": {"__FILE__": "../files/titanic.csv"}, "expected_answer.txt": {"__ANSWER__": "@correlation_coefficient[-0.123]"}, "format.txt": {"__FORMAT__": "@correlation_coefficient[c_value] where \"c_value\" is a number between -1 and 1, rounded to three decimal places."}, "question.txt": {"__QUESTION__": "Calculate the Pearson correlation coefficient between the age and fare variables for passengers who survived and were in first class."}}}
{"id": "214", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "{\n- Use Pearson correlation for the correlation analysis.\n- Assess the strength of the correlation between each pair of variables. Consider correlations to be weak if |r| < 0.3, moderate if 0.3 <= |r| < 0.5, and strong if |r| >= 0.5. \n}"}, "file.txt": {"__FILE__": "../files/fb_articles_20180822_20180829_df.csv"}, "expected_answer.txt": {"__ANSWER__": "@pos_length_corr[-0.35],@neu_length_corr[0.42]"}, "format.txt": {"__FORMAT__": "{\n@neg_length_corr[neg_length_correlation]\n@neu_length_corr[neu_length_correlation]\n@pos_length_corr[pos_length_correlation]\nwhere \"neg_length_correlation\", \"neu_length_correlation\" and \"pos_length_correlation\" are decimal numbers between -1 and 1 (rounded to 2 decimal places) representing the strength of the correlation between the sentiment scores and the article length."}, "question.txt": {"__QUESTION__": "2. Perform a correlation analysis between the sentiment scores (\"neg\", \"neu\", \"pos\") and the article length (\"text\" column non-space character count) for articles published by the source \"ABC News\". Identify any significant correlations between the variables and provide a brief explanation of the findings."}}}
{"id": "244", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Assess whether the data is normally distributed using the Shapiro-Wilk test for normality with a significance level (alpha) of 0.05. Exclude the player with a missing value of home runs in your calculations. \nIf the p-value is less than 0.05, report that the distribution is not normal. If the p-value is greater than or equal to 0.05, report that the distribution is normal."}, "file.txt": {"__FILE__": "../files/baseball_data.csv"}, "expected_answer.txt": {"__ANSWER__": "@normality_test[not_normal]"}, "format.txt": {"__FORMAT__": "@p_value[p_value] @normality_test[normality_test] where \"p_value\" is a number between 0 and 1, rounded to four decimal places. \"normality_test\" is a string which can either be \"normal\" or \"not_normal\" based on the conditions specified in the constraints."}, "question.txt": {"__QUESTION__": "Are the number of home runs hit by the players normally distributed?"}}}
{"id": "308", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Only the titles 'Mr.', 'Mrs.', 'Miss.' and 'Master.' should be considered. Titles that do not fall within these four categories should be eliminated."}, "file.txt": {"__FILE__": "../files/titanic.csv"}, "expected_answer.txt": {"__ANSWER__": "@average_fare_Mrs[45.14],@average_fare_Mr[24.44]"}, "format.txt": {"__FORMAT__": "@average_fare_Mr[value1], @average_fare_Mrs[value2], @average_fare_Miss[value3], @average_fare_Master[value4], where value1, value2, value3, and value4 represent the average fares for 'Mr.', 'Mrs.', 'Miss.', and 'Master.', respectively. All values should be rounded to two decimal places."}, "question.txt": {"__QUESTION__": "Use feature engineering techniques to create a new variable \"Title\" by extracting the title from the Name column (e.g., \"Mr.\", \"Mrs.\", \"Miss\"). Only consider the following titles: 'Mr.', 'Mrs.', 'Miss.' and 'Master.' (titles followed by a dot). Then, calculate the average fare for each unique title to two decimal places."}}}
{"id": "372", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Both mean and median should be calculated by the built-in Python function, not manually. The result should be rounded to two decimal places."}, "file.txt": {"__FILE__": "../files/2014_q4.csv"}, "expected_answer.txt": {"__ANSWER__": "@mean[21144.08]"}, "format.txt": {"__FORMAT__": "@mean[mean_value] @median[median_value] where \"mean_value\" and \"median_value\" are numbers, rounded to two decimal places."}, "question.txt": {"__QUESTION__": "1. Find the mean and median of the \"Trips over the past 24-hours (midnight to 11:59pm)\" column."}}}
{"id": "423", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between the new feature 'Volatility' and trading volume. Consider the relationship to be positive if the r value is greater than or equal to 0.5. Consider the relationship to be negative if the r value is less than or equal to -0.5. Otherwise, report that there is no significant correlation."}, "file.txt": {"__FILE__": "../files/bitconnect_price.csv"}, "expected_answer.txt": {"__ANSWER__": "@relationship_type[none],@correlation_coefficient[-0.09]"}, "format.txt": {"__FORMAT__": "@correlation_coefficient[correlation_coefficient], @relationship_type[type] where \"correlation_coefficient\" is a number between -1 and 1, rounded to two decimal places. \"type\" is a string that can be \"positive\", \"negative\", or \"none\" based on the conditions specified in the constraints."}, "question.txt": {"__QUESTION__": "2. Perform feature engineering on the given dataset to create a new feature 'Volatility' that is calculated using the formula: Volatility = (High Price - Low Price) / Open Price. What is the Pearson correlation coefficient between Volatility and trading volume? Interpret the result."}}}
{"id": "424", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Use a Random Forest Classifier for the model and split the data into a 75% training set and 25% test set. Use out-of-the-box settings for the model. The accuracy should be calculated on the test set. Measures of feature importance should be based on the Gini importance or mean decrease impurity."}, "file.txt": {"__FILE__": "../files/bitconnect_price.csv"}, "expected_answer.txt": {"__ANSWER__": "@feature2[High]"}, "format.txt": {"__FORMAT__": "@accuracy[accuracy], @feature1[feature_name], @feature2[feature_name], @feature3[feature_name] where \"accuracy\" is the model accuracy rounded to two decimal places. \"feature_name\" is the name of the feature from the dataset, listed by the importance in descending order."}, "question.txt": {"__QUESTION__": "3. Develop a machine learning model to classify the asset or commodity into different price categories (low, medium, high) based on the opening, high, and low prices. The boundaries for the categories are: Low(< 500), Medium(500 - 1000), High(> 1000). What are the accuracy of the model and the top three contributing features to the classification?"}}}
{"id": "426", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Firstly, identify the storm with the highest maximum storm category, neglecting any ties. If there are multiple storms with the same highest maximum storm category, choose the one that appears first in the given dataset. Then find the maximum sustained wind speed corresponding to this particular storm."}, "file.txt": {"__FILE__": "../files/cost_data_with_errors.csv"}, "expected_answer.txt": {"__ANSWER__": "@max_wind_speed[156.42]"}, "format.txt": {"__FORMAT__": "@max_wind_speed[number] where \"number\" is a float with two decimal places indicating the highest wind speed recorded for the storm with the highest maximum storm category."}, "question.txt": {"__QUESTION__": "2. What is the maximum sustained wind speed recorded during the storm with the highest maximum storm category?"}}}
{"id": "429", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "{\nCalculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between maximum storm category and damage in USD.\nUse a two-tailed test with a significance level (alpha) of 0.05 to assess the significance of the correlation. \nReport the p-value associated with the correlation test.\nIf the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5, infer the relationship to be linear.\nIf the p-value is less than 0.05 and the absolute value of r is less than 0.5, infer the relationship to be nonlinear.\nIf the p-value is greater than or equal to 0.05, report that there is no significant correlation.\n}"}, "file.txt": {"__FILE__": "../files/cost_data_with_errors.csv"}, "expected_answer.txt": {"__ANSWER__": "@correlation_coefficient[0.19],@relationship_type[nonlinear],@p_value[0.0]"}, "format.txt": {"__FORMAT__": "{\n@correlation_coefficient[r_value]\n@p_value[p_value]\n@relationship_type[relationship_type]\nwhere \"r_value\" is a number between -1 and 1, rounded to 2 decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to 4 decimal places.\nwhere \"relationship_type\" is a string that can either be \"linear\", \"nonlinear\", or \"none\" based on the conditions specified in the constraints."}, "question.txt": {"__QUESTION__": "2. Is there a correlation between the maximum storm category achieved by a storm and the recorded damage in USD? If so, what is the strength and direction of the correlation?"}}}
{"id": "449", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Use the Shapiro-Wilk test to determine if the distribution is normal. Accept the null hypothesis that the data is normally distributed if the p-value is greater than 0.05, and reject it otherwise."}, "file.txt": {"__FILE__": "../files/baro_2015.csv"}, "expected_answer.txt": {"__ANSWER__": "@normal_distribution[no]"}, "format.txt": {"__FORMAT__": "@shapiro_wilk_pvalue[p_value] @normal_distribution[yes/no] where \"p_value\" is a number between 0 and 1, rounded to four decimal places and \"normal_distribution\" is a string that can either be \"yes\" if p_value > 0.05 or \"no\" if p_value <= 0.05"}, "question.txt": {"__QUESTION__": "1. What is the distribution of wind speeds (WINDSPEED) in the dataset? Is it normally distributed?"}}}
{"id": "468", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "You are to use the Interquartile Range (IQR) method for outlier detection. Calculate the IQR as Q3 (75th percentile) - Q1 (25th percentile) for the 'Assault' category. Outliers are considered as values lying below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR."}, "file.txt": {"__FILE__": "../files/arrest_expungibility.csv"}, "expected_answer.txt": {"__ANSWER__": "@number_of_outliers[0]"}, "format.txt": {"__FORMAT__": "@number_of_outliers[number] where \"number\" is a positive integer denoting the number of outliers"}, "question.txt": {"__QUESTION__": "2. Are there any outliers in the age distribution of offenders in 'Assault' category, according to the IQR method? If yes, report the number of outliers."}}}
{"id": "669", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Identify an outlier as any value that falls below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 are the first and third quartiles, respectively, and IQR is the interquartile range (Q3 - Q1). Calculate the mean and standard deviation to two decimal places."}, "file.txt": {"__FILE__": "../files/my_test_01.csv"}, "expected_answer.txt": {"__ANSWER__": "@standard_deviation[1.54],@mean[3.73]"}, "format.txt": {"__FORMAT__": "@mean[mean_value] where \"mean_value\" is a float rounded to two decimal places. @standard_deviation[standard_deviation_value] where \"standard_deviation_value\" is a float rounded to two decimal places."}, "question.txt": {"__QUESTION__": "Identify and remove any outliers in the MedInc column of the provided dataset using the IQR method. Then calculate the mean and standard deviation of the cleaned MedInc column."}}}
{"id": "710", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Assume all values in the \"JAMES LOGAN\" column are numeric, and convert strings to numbers if necessary. Ignore any rows where \"JAMES LOGAN\" is missing or cannot be converted to a number. Use pandas `mean()` function to calculate the mean."}, "file.txt": {"__FILE__": "../files/Current_Logan.csv"}, "expected_answer.txt": {"__ANSWER__": "@mean_wins[2.6]"}, "format.txt": {"__FORMAT__": "@mean_wins[mean]"}, "question.txt": {"__QUESTION__": "1. What is the mean number of wins in the \"JAMES LOGAN\" column?"}}}
{"id": "733", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Calculate the logarithm with base 10.\nWhile calculating the logarithm, assume all GDP per capita figures are positive."}, "file.txt": {"__FILE__": "../files/gapminder_cleaned.csv"}, "expected_answer.txt": {"__ANSWER__": "@has_nan_values_in_new_feature[False],@new_feature_mean[3.54],@new_feature_std[0.54]"}, "format.txt": {"__FORMAT__": "@has_nan_values_in_new_feature[boolean]\n@new_feature_mean[mean]\n@new_feature_std[std]\nwhere \"boolean\" is True or False, indicating whether there are NaN values in the newly created feature.\nwhere \"mean\" is a number (rounded to 2 decimal places) representing the mean of the newly created feature.\nwhere \"std\" is a number (rounded to 2 decimal places) representing the standard deviation of the newly created feature."}, "question.txt": {"__QUESTION__": "Apply feature engineering techniques to create a new feature in the dataset that represents the GDP per capita in logarithmic scale (base 10). Implement this feature transformation using Python code."}}}
{"id": "743", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "{\nFirst, no assumptions should be made about the presence of missing values in the Education column. Check first if there are any such values even though the scenario information states that there are none.\nFor missing value imputation, use the mode (most frequently occurring value) to fill the missing gaps in the Education column.\nFor normalization of \"Income\" and \"Balance\", use Min-Max normalization method whose calculation is given by: (X - min(X)) / (max(X) - min(X)) where X denotes a value from the respective column.\n}"}, "file.txt": {"__FILE__": "../files/Credit.csv"}, "expected_answer.txt": {"__ANSWER__": "@income_normalization[10.35, 186.63, /mnt/data/Credit_Income_Normalized.csv]"}, "format.txt": {"__FORMAT__": "{\n@education_mode[education_mode]\n@income_normalization[income_min_val, income_max_val, income_normalized_file_path]\n@balance_normalization[balance_min_val, balance_max_val, balance_normalized_file_path]\nwhere \"education_mode\" is the mode of the \"Education\" column, an integer.\nwhere \"income_min_val\" and \"income_max_val\" are the minimum and maximum values respectively of the \"Income\" column before normalization, rounded to two decimal places.\nwhere \"balance_min_val\" and \"balance_max_val\" are the minimum and maximum values respectively of the \"Balance\" column before normalization, rounded to two decimal places.\nwhere \"income_normalized_file_path\" is the file path for the CSV file with the normalized \"Income\" column.\nwhere \"balance_normalized_file_path\" is the file path for the CSV file with the normalized \"Balance\" column."}, "question.txt": {"__QUESTION__": "Perform a comprehensive data preprocessing on the Credit.csv file by handling missing values in the \"Education\" column using imputation with the most frequent value, and normalizing the \"Income\" and \"Balance\" columns."}}}
{"id": "757", "template": "../Templates/TwoAgents", "substitutions": {"constraint.txt": {"__CONSTRAINT__": "Calculate the interquartile range (IQR) for obs_value. Any value that falls below Q1 - 1.5*IQR or above Q3 + 1.5*IQR is considered an outlier. Count the number of outliers according to this method."}, "file.txt": {"__FILE__": "../files/weather_data_1864.csv"}, "expected_answer.txt": {"__ANSWER__": "@outlier_count[25]"}, "format.txt": {"__FORMAT__": "@outlier_count[total_outlier] where \"total_outlier\" is an integer representing the number of outliers. If there are no outliers, output @outlier_status[\"No Outliers Detected\"]"}, "question.txt": {"__QUESTION__": "3. Are there any outliers in the observation values (obs_value) column? If yes, how many outliers are there using the interquartile range method?"}}}
